# ğŸ¤– AI Models Comparison Platform

Una plataforma completa para comparar mÃºltiples modelos de LLM (Large Language Models) de forma concurrente, con sistema de evaluaciÃ³n inteligente y mÃ©tricas detalladas.

## âœ¨ CaracterÃ­sticas

### ğŸ¯ **Funcionalidades Principales**
- **ComparaciÃ³n Concurrente**: Ejecuta mÃºltiples modelos en paralelo usando `asyncio.gather`
- **EvaluaciÃ³n Inteligente**: Sistema de evaluaciÃ³n basado en 3 criterios (PrecisiÃ³n, Completitud, Claridad)
- **MÃ©tricas Detalladas**: Scores individuales, promedio, consistencia y tiempo de ejecuciÃ³n
- **Interfaz Moderna**: DiseÃ±o inspirado en Cursor IDE + Stripe con UX optimizada
- **Arquitectura Modular**: FÃ¡cil extensiÃ³n para nuevos mÃ³dulos (Classification, Extraction, etc.)
- **Sistema de Recomendaciones**: Sugerencias inteligentes basadas en resultados

### ğŸ”§ **TecnologÃ­as**
- **Backend**: FastAPI + Pydantic + Uvicorn
- **Frontend**: Vanilla JavaScript ES6+ + CSS Grid + Flexbox
- **LLM Providers**: OpenAI, Anthropic, Google
- **Concurrencia**: `asyncio.gather` para ejecuciÃ³n paralela
- **Deployment**: GitHub + Docker + Gunicorn
- **CI/CD**: GitHub Actions con testing automÃ¡tico

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos
- Python 3.8+
- API Keys de los proveedores de LLM

### InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone https://github.com/asertas14/ai-models-comparison-platform.git
cd ai-models-comparison-platform
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno**
```bash
cp env.example .env
# Editar .env con tus API keys
```

5. **Ejecutar la aplicaciÃ³n**
```bash
python main.py
```

La aplicaciÃ³n estarÃ¡ disponible en `http://localhost:8000`

## ğŸ”‘ ConfiguraciÃ³n de API Keys

Crea un archivo `.env` en la raÃ­z del proyecto:

```bash
# OpenAI
OPENAI_API_KEY=tu_openai_api_key_aqui

# Anthropic (opcional)
ANTHROPIC_API_KEY=tu_anthropic_api_key_aqui

# Google (opcional)
GOOGLE_API_KEY=tu_google_api_key_aqui

# ConfiguraciÃ³n del servidor
DEBUG=true
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO
```

## ğŸ“ Estructura del Proyecto

```
ai-models-comparison-platform/
â”œâ”€â”€ .github/                       # GitHub Actions & Templates
â”‚   â”œâ”€â”€ workflows/ci.yml           # CI/CD Pipeline
â”‚   â””â”€â”€ ISSUE_TEMPLATE/            # Bug & Feature Templates
â”œâ”€â”€ .devcontainer/                 # GitHub Codespaces
â”‚   â””â”€â”€ devcontainer.json          # Dev environment config
â”œâ”€â”€ app/                          # Backend FastAPI
â”‚   â”œâ”€â”€ llm/                      # MÃ³dulo LLM genÃ©rico
â”‚   â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n LLM
â”‚   â”‚   â”œâ”€â”€ models.py              # Modelos Pydantic
â”‚   â”‚   â”œâ”€â”€ service.py             # Servicio LLM
â”‚   â”‚   â””â”€â”€ router.py              # Rutas API
â”‚   â”œâ”€â”€ summarization/             # MÃ³dulo Summarization
â”‚   â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n
â”‚   â”‚   â”œâ”€â”€ models.py              # Modelos
â”‚   â”‚   â”œâ”€â”€ service.py             # Servicio
â”‚   â”‚   â”œâ”€â”€ evaluator.py          # Evaluador inteligente
â”‚   â”‚   â””â”€â”€ router.py              # Rutas
â”‚   â””â”€â”€ config.py                  # ConfiguraciÃ³n global
â”œâ”€â”€ static/                        # Frontend
â”‚   â”œâ”€â”€ css/                       # Estilos
â”‚   â”‚   â”œâ”€â”€ themes/                # Temas (Cursor + Stripe)
â”‚   â”‚   â””â”€â”€ modules/                # Estilos por mÃ³dulo
â”‚   â””â”€â”€ js/                        # JavaScript ES6+
â”‚       â”œâ”€â”€ core/                  # Core frontend (API, Router, State)
â”‚       â””â”€â”€ modules/               # MÃ³dulos frontend
â”œâ”€â”€ templates/                     # Templates HTML (Jinja2)
â”œâ”€â”€ main.py                        # Punto de entrada FastAPI
â”œâ”€â”€ requirements.txt               # Dependencias Python
â”œâ”€â”€ env.example                    # Variables de entorno ejemplo
â”œâ”€â”€ CONTRIBUTING.md                # GuÃ­a de contribuciÃ³n
â”œâ”€â”€ LICENSE                        # MIT License
â””â”€â”€ README.md                      # Este archivo
```

## ğŸ¨ Arquitectura

### Backend (FastAPI)
- **Arquitectura Modular**: Cada funcionalidad en su propio mÃ³dulo
- **Concurrencia**: `asyncio.gather` para ejecuciÃ³n paralela
- **EvaluaciÃ³n**: Sistema de reconstrucciÃ³n de texto para evaluar resÃºmenes
- **API RESTful**: Endpoints bien documentados

### Frontend (Vanilla JS)
- **Modular**: JavaScript organizado por mÃ³dulos
- **Responsive**: CSS Grid + Flexbox
- **Tema**: Inspirado en Cursor IDE + Stripe
- **Estado**: GestiÃ³n de estado centralizada

## ğŸ”¬ Sistema de EvaluaciÃ³n Inteligente

### MÃ©todo de EvaluaciÃ³n por Criterios
1. **GeneraciÃ³n**: Se generan 3 resÃºmenes por modelo (optimizado para costos)
2. **EvaluaciÃ³n Individual**: Cada resumen se evalÃºa en 3 criterios (1-5 puntos cada uno):
   - **PrecisiÃ³n**: QuÃ© tan preciso es el resumen
   - **Completitud**: QuÃ© tan completo es el resumen
   - **Claridad**: QuÃ© tan claro y comprensible es
3. **ReconstrucciÃ³n General**: Un modelo evaluador reconstruye el texto original
4. **Similitud**: Se calcula la similitud Jaccard entre original y reconstruido
5. **Consistencia**: Mide la consistencia entre los 3 resÃºmenes

### MÃ©tricas
- **Score Individual**: 3-15 puntos por resumen (3 criterios Ã— 1-5 puntos)
- **Score Promedio**: Promedio de todos los resÃºmenes del modelo
- **Score Mejor**: Mejor resumen individual del modelo
- **Score Peor**: Peor resumen individual del modelo
- **Consistencia**: QuÃ© tan consistentes son los resÃºmenes entre sÃ­
- **Tiempo de EjecuciÃ³n**: Tiempo de ejecuciÃ³n por modelo

## ğŸš€ Deployment

### Desarrollo Local

```bash
# Clonar y configurar
git clone https://github.com/asertas14/ai-models-comparison-platform.git
cd ai-models-comparison-platform
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Configurar variables de entorno
cp env.example .env
# Editar .env con tus API keys

# Ejecutar
python main.py
```

### Deployment en Servidor

```bash
# Usar gunicorn para producciÃ³n
pip install gunicorn
gunicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Docker (Opcional)

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["python", "main.py"]
```

## ğŸ“Š Uso de la AplicaciÃ³n

### 1. Dashboard
- **EstadÃ­sticas**: Modelos disponibles, proveedores activos
- **Acciones RÃ¡pidas**: NavegaciÃ³n a mÃ³dulos
- **Estado del Sistema**: Health check y configuraciÃ³n

### 2. Summarization Module
- **SelecciÃ³n de Modelos**: Filtros por proveedor, bÃºsqueda inteligente
- **ConfiguraciÃ³n LLM**: Temperature, tokens, top-p, top-k
- **Input de Texto**: Textarea con contador de palabras (mÃ­nimo 100 caracteres)
- **ComparaciÃ³n**: Ejecuta modelos en paralelo (3 muestras por modelo)
- **Resultados**: 
  - Scores individuales (PrecisiÃ³n, Completitud, Claridad)
  - Score promedio, mejor y peor
  - ReconstrucciÃ³n general del texto
  - ResÃºmenes individuales con evaluaciÃ³n detallada
  - Sistema de recomendaciones inteligentes

### 3. Filtros y BÃºsqueda
- **Por Proveedor**: OpenAI, Google, Anthropic
- **BÃºsqueda**: Por nombre de modelo o proveedor
- **SelecciÃ³n Persistente**: Se mantiene al cambiar filtros

## ğŸ”§ API Endpoints

### LLM Module
- `GET /llm/models` - Lista modelos disponibles
- `GET /llm/config` - ConfiguraciÃ³n LLM
- `POST /llm/test/{model}` - Probar modelo especÃ­fico

### Summarization Module
- `POST /summarization/compare` - Comparar modelos
- `GET /summarization/config` - ConfiguraciÃ³n
- `POST /summarization/test` - Probar resumen simple

### Sistema
- `GET /health` - Health check
- `GET /` - Dashboard principal

## ğŸ§ª Testing

### Pruebas AutomÃ¡ticas (GitHub Actions)
```bash
# Probar API
curl http://localhost:8000/health

# Probar modelos
curl http://localhost:8000/llm/models

# Probar configuraciÃ³n
curl http://localhost:8000/summarization/config

# Probar resumen simple
curl -X POST "http://localhost:8000/summarization/test" \
  -H "Content-Type: application/json" \
  -d '{"text":"Este es un texto de prueba para verificar que el sistema funciona correctamente. Debe tener al menos 100 caracteres para cumplir con los requisitos mÃ­nimos del sistema.","model":"gpt-3.5-turbo","max_words":20}'
```

### Pruebas Manuales
1. **NavegaciÃ³n**: Dashboard â†’ Summarization (usando router client-side)
2. **SelecciÃ³n**: Filtrar por proveedor, buscar modelos, seleccionar mÃºltiples
3. **ConfiguraciÃ³n**: Ajustar parÃ¡metros LLM (temperature, tokens, etc.)
4. **ComparaciÃ³n**: Ingresar texto (mÃ­nimo 100 caracteres) + modelos seleccionados
5. **Resultados**: Verificar scores individuales, promedio, reconstrucciÃ³n y recomendaciones

## ğŸ› ï¸ Desarrollo

### Agregar Nuevo MÃ³dulo
1. Crear directorio en `app/`
2. Implementar `config.py`, `models.py`, `service.py`, `router.py`
3. Agregar rutas en `main.py`
4. Crear frontend en `static/js/modules/`
5. Agregar estilos en `static/css/modules/`

### Agregar Nuevo Proveedor LLM
1. Implementar cliente en `app/llm/service.py`
2. Agregar configuraciÃ³n en `app/config.py`
3. Actualizar detecciÃ³n de proveedor
4. Probar con API key

## ğŸ“ˆ Roadmap

### PrÃ³ximas Funcionalidades
- [ ] **Extraction Module**: ExtracciÃ³n de campos estructurados
- [ ] **Documents Module**: Procesamiento de PDF, Word, Excel
- [ ] **Classification Module**: ClasificaciÃ³n de texto
- [ ] **Sentiment Module**: AnÃ¡lisis de sentimientos
- [ ] **GrÃ¡ficos Interactivos**: VisualizaciÃ³n de resultados
- [ ] **Historial**: Guardar comparaciones anteriores
- [ ] **Export**: Exportar resultados a PDF/Excel

### Mejoras TÃ©cnicas
- [ ] **Caching**: Redis para cache de resultados
- [ ] **Database**: PostgreSQL para persistencia
- [ ] **Authentication**: Sistema de usuarios
- [ ] **Rate Limiting**: LÃ­mites de uso por usuario
- [ ] **Monitoring**: MÃ©tricas y alertas

## ğŸ¤ Contribuir

### GuÃ­a de ContribuciÃ³n
Ver [CONTRIBUTING.md](CONTRIBUTING.md) para la guÃ­a completa de contribuciÃ³n.

### Proceso RÃ¡pido
1. **Fork** el repositorio
2. **Crear** feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** cambios (`git commit -m 'âœ¨ Add some AmazingFeature'`)
4. **Push** al branch (`git push origin feature/AmazingFeature`)
5. **Abrir** Pull Request

### GitHub Features
- **Issues**: Usar templates para bugs y features
- **CI/CD**: Testing automÃ¡tico en cada PR
- **Codespaces**: Desarrollo en la nube
- **Actions**: Workflows automÃ¡ticos

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- **FastAPI** - Framework web moderno y rÃ¡pido
- **OpenAI** - Modelos GPT
- **Anthropic** - Modelos Claude
- **Google** - Modelos Gemini
- **Cursor IDE** - InspiraciÃ³n para el diseÃ±o
- **Stripe** - InspiraciÃ³n para componentes

## ğŸ“ Soporte

- **Issues**: [GitHub Issues](https://github.com/asertas14/ai-models-comparison-platform/issues)
- **Discussions**: [GitHub Discussions](https://github.com/asertas14/ai-models-comparison-platform/discussions)
- **DocumentaciÃ³n**: [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ”— Enlaces Ãštiles

- **Repositorio**: https://github.com/asertas14/ai-models-comparison-platform
- **Issues**: https://github.com/asertas14/ai-models-comparison-platform/issues
- **Releases**: https://github.com/asertas14/ai-models-comparison-platform/releases
- **Actions**: https://github.com/asertas14/ai-models-comparison-platform/actions

---

**Desarrollado con â¤ï¸ para la comunidad de AI/ML**
